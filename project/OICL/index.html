<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Implicit Counterfactual Learning for Audio-Visual Segmentation</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">OICL</h1>
    <h1 class="nerf_subheader_v2">Towards Omni-Implicit Counterfactual Learning in Audio-Visual Segmentation</h1>
        <h3 class="nerf_subheader_v2">TPAMI Submitted</h3>
    <hr>
    
    </br></br>
    
    <!-- <p class="authors">
        <a href="https://scholar.google.com/citations?hl=en&user=EKNIWTIAAAAJ">Mingfeng Zha</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=V08v5OEAAAAJ">Guoqing Wang*</a>,
        <a href="https://scholar.google.com/citations?user=6MUsCT4AAAAJ&hl=en&oi=sra">Tianyu Li</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=vIr3ICQAAAAJ">Peng Wang</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=XzZSbxAAAAAJ">Yunqiang Pei</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=YjSHPjcAAAAJ">Jingcai Guo</a>,
        <a href="https://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
        <a href="https://scholar.google.com.au/citations?user=krryaDkAAAAJ&hl=en">Heng Tao Shen</a>
        
        </br></br>
    </p> -->

    <div class="container text-center">
    <p class="authors" style="font-size: 1.2rem; font-weight: 500;">
        <a href="https://scholar.google.com/citations?hl=en&user=EKNIWTIAAAAJ">Mingfeng Zha</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=V08v5OEAAAAJ">Guoqing Wang</a><sup>*</sup>,
        <a href="https://scholar.google.com/citations?user=6MUsCT4AAAAJ&hl=en&oi=sra">Tianyu Li</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=vIr3ICQAAAAJ">Peng Wang</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=XzZSbxAAAAAJ">Yunqiang Pei</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=YjSHPjcAAAAJ">Jingcai Guo</a>,
        <a href="https://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
        <a href="https://scholar.google.com.au/citations?user=krryaDkAAAAJ&hl=en">Heng Tao Shen</a>
    </p>

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/uestc.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
        <p style="font-size: 1.1rem; color: #555; margin: 0;">
            University of Electronic Science and Technology of China
        </p>
    </div>

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/polyu.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
        <p style="font-size: 1.1rem; color: #555; margin: 0;">
            The Hong Kong Polytechnic University
        </p>
    </div>

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/tongji.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
        <p style="font-size: 1.1rem; color: #555; margin: 0;">
            Tongji University
        </p>
    </div>

    </div>

    <!-- </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zha_Implicit_Counterfactual_Learning_for_Audio-Visual_Segmentation_ICCV_2025_paper.pdf">Conference Version</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1plklRpsYbCJsHJCV8wq4kvQxKOITVXhq/view?usp=drive_link">Segmentation Results (Google Drive)</a>
        <a class="btn btn-primary" href="https://pan.baidu.com/s/1fARfFl3jlfN2_Pb7VF-QNQ?pwd=143y">Segmentation Results (Baidu)</a>
        <a class="btn btn-primary" href=" ">Comparison Results (Google Drive)</a>
        <a class="btn btn-primary" href="">Comparison Results (Baidu)</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1Tvse5DIYFQa6e_dmnba_O0U7fKKin_TW/view?usp=drive_link">Evaluation Code</a>
    </div>
    </br></br> -->

    </br></br>
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12">
                <div class="btn-group d-flex flex-wrap justify-content-center" role="group" aria-label="Top menu">
                    <a class="btn btn-primary m-1" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zha_Implicit_Counterfactual_Learning_for_Audio-Visual_Segmentation_ICCV_2025_paper.pdf">Conference Version</a>
                    <a class="btn btn-primary m-1" href="https://drive.google.com/file/d/1jNyxI91mD-shBpbb0Se28FPRzOCwMXWc/view?usp=drive_link">Segmentation Results (Google Drive)</a>
                    <a class="btn btn-primary m-1" href="https://pan.baidu.com/s/1fARfFl3jlfN2_Pb7VF-QNQ?pwd=143y">Segmentation Results (Baidu)</a>
                    <a class="btn btn-primary m-1" href="https://drive.google.com/file/d/1v3jE8Hr0_okEJgOMyf-pVAaLtgu89FjU/view?usp=drive_link">Comparison Results (Google Drive)</a>
                    <a class="btn btn-primary m-1" href="https://pan.baidu.com/s/1RH34CrRDQV4g_yyeV7j0FQ?pwd=pi6r">Comparison Results (Baidu)</a>
                    <a class="btn btn-primary m-1" href="https://drive.google.com/file/d/1Tvse5DIYFQa6e_dmnba_O0U7fKKin_TW/view?usp=drive_link">Evaluation Code</a>
                </div>
            </div>
        </div>
    </div>
    </br></br>


    <!-- <p class="paragraph-3 nerf_text" style="text-align: center;">
        The code and other resources will be made publicly available after the paper is accepted. Thank you! :)
    </p> -->

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/tongji.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
    <p style="font-size: 1.1rem; color: #555; margin: 0;">
            More resources will be released upon acceptance :)
    </p>
    </div>

</div>



<div class="container">

    <div class="row">
        <div class="col-md-10 offset-md-1">
            <h2 class="text-center">Abstract</h2>
            <p class="text-justify" style="font-size: 1.1em; line-height: 1.6;">
                Audio-visual segmentation (AVS) aims to segment objects in videos based on audio cues. Existing AVS methods are primarily designed to enhance interaction efficiency but pay limited attention to modality representation discrepancies and imbalances. In this work, we propose Omni-Implicit Counterfactual Learning (OICL), a unified causal inference paradigm for unbiased and fine-grained multimodal reasoning in AVS. To establish a semantically grounded shared embedding space, we introduce Multi-granularity Implicit Text (MIT) representations at video-, segment-, and frame-levels, which bridge low-level audio features and visual semantics. Moreover, we augment implicit cues with MLLM-driven fine-grained scene descriptions, providing explicit semantic anchors that improve identifiability and alignment stability. To mitigate visual dominance and biased cross-modal knowledge transfer, we further develop Hierarchical Semantic Counterfactual (HSC), which intervene on semantic representations to promote orthogonal latent decompositions and generate diverse counterfactual variations without architectural modification. Complementarily, Hierarchical Perceptual Counterfactual (HPC) are introduced to perform modality-aware interventions along audio and visual pathways, enhancing causal robustness under distribution shifts. Building upon these counterfactual constructions, we formulate Distribution-informed Cooperative Contrastive Learning (DCCL), a structured objective that jointly models factualâ€“counterfactual, intra-modal, and inter-modal relationships. This cooperative contrastive formulation enforces cohesive alignment while explicitly decoupling causal and spurious correlations. Extensive experiments on six benchmarks demonstrate that OICL consistently achieves state-of-the-art performance. Furthermore, few-shot and open-vocabulary evaluations reveal promising out-of-distribution generalization. 
            </p>
        </div>
    </div>

    <hr>
    <h2 class="text-center">Demo Videos</h2>
    <div class="row">
        <div class="col-md-4">
            <p class="text-center">Bassoon</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/bassoon.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Marimba</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/marimba.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Airplane</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/airplane.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Harp</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/harp.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Keyboard</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/keyboard.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Guitar & Man</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/guitar_man.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Accordion & Guitar</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/accordion_guitar.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Cello & Violin</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/cello_violin.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Cello & bassoon & Violin</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/cello_bassoon_violin.mp4" type="video/mp4">
            </video>
        </div>

    </div>

    <hr>
    <div class="container">
        <h2>BibTeX</h2>
        <pre style="background-color: #f5f5f5; padding: 20px; border-radius: 5px;"><code>@article{zha2025oicl,
        title={Towards Omni-Implicit Counterfactual Learning in Audio-Visual Segmentation},
        author={Zha, Mingfeng and Wang, Guoqing and Li, Tianyu and Wang, Peng and Pei, Yunqiang and Guo, Jingcai and Yang, Yang and Shen, Heng Tao},
        journal={preprint},
        year={2026}
    }</code></pre>
    </div>

    </br></br>
    <div class="section">


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
