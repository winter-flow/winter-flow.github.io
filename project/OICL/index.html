<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Towards Omni-Implicit Counterfactual Learning in Audio-Visual Segmentation</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">OICL</h1>
    <h1 class="nerf_subheader_v2">Towards Omni-Implicit Counterfactual Learning in Audio-Visual Segmentation</h1>
        <h3 class="nerf_subheader_v2">TPAMI Submitted (ICCV25 Extension)</h3>
    <hr>
    
    </br></br>
    
    <!-- <p class="authors">
        <a href="https://scholar.google.com/citations?hl=en&user=EKNIWTIAAAAJ">Mingfeng Zha</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=V08v5OEAAAAJ">Guoqing Wang*</a>,
        <a href="https://scholar.google.com/citations?user=6MUsCT4AAAAJ&hl=en&oi=sra">Tianyu Li</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=vIr3ICQAAAAJ">Peng Wang</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=XzZSbxAAAAAJ">Yunqiang Pei</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=YjSHPjcAAAAJ">Jingcai Guo</a>,
        <a href="https://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
        <a href="https://scholar.google.com.au/citations?user=krryaDkAAAAJ&hl=en">Heng Tao Shen</a>
        
        </br></br>
    </p> -->

    <div class="container text-center">
    <p class="authors" style="font-size: 1.2rem; font-weight: 500;">
        <a href="https://winter-flow.github.io/">Mingfeng Zha</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=V08v5OEAAAAJ">Guoqing Wang</a><sup>*</sup>,
        <a href="https://scholar.google.com/citations?user=6MUsCT4AAAAJ&hl=en&oi=sra">Tianyu Li</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=vIr3ICQAAAAJ">Peng Wang</a>,
        <a href="https://scholar.google.com.au/citations?hl=en&user=XzZSbxAAAAAJ">Yunqiang Pei</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=YjSHPjcAAAAJ">Jingcai Guo</a>,
        <a href="https://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
        <a href="https://scholar.google.com.au/citations?user=krryaDkAAAAJ&hl=en">Heng Tao Shen</a>
    </p>

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/uestc.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
        <p style="font-size: 1.1rem; color: #555; margin: 0;">
            University of Electronic Science and Technology of China
        </p>
    </div>

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/polyu.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
        <p style="font-size: 1.1rem; color: #555; margin: 0;">
            The Hong Kong Polytechnic University
        </p>
    </div>

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/tongji.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
        <p style="font-size: 1.1rem; color: #555; margin: 0;">
            Tongji University
        </p>
    </div>

    </div>

    <!-- </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zha_Implicit_Counterfactual_Learning_for_Audio-Visual_Segmentation_ICCV_2025_paper.pdf">Conference Version</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1plklRpsYbCJsHJCV8wq4kvQxKOITVXhq/view?usp=drive_link">Segmentation Results (Google Drive)</a>
        <a class="btn btn-primary" href="https://pan.baidu.com/s/1fARfFl3jlfN2_Pb7VF-QNQ?pwd=143y">Segmentation Results (Baidu)</a>
        <a class="btn btn-primary" href=" ">Comparison Results (Google Drive)</a>
        <a class="btn btn-primary" href="">Comparison Results (Baidu)</a>
        <a class="btn btn-primary" href="https://drive.google.com/file/d/1Tvse5DIYFQa6e_dmnba_O0U7fKKin_TW/view?usp=drive_link">Evaluation Code</a>
    </div>
    </br></br> -->

    </br></br>
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12">
                <div class="btn-group d-flex flex-wrap justify-content-center" role="group" aria-label="Top menu">
                    <a class="btn btn-primary m-1" href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zha_Implicit_Counterfactual_Learning_for_Audio-Visual_Segmentation_ICCV_2025_paper.pdf">Conference Version</a>
                    <a class="btn btn-primary m-1" href="https://drive.google.com/file/d/1jNyxI91mD-shBpbb0Se28FPRzOCwMXWc/view?usp=drive_link">Segmentation Results (Google Drive)</a>
                    <a class="btn btn-primary m-1" href="https://pan.baidu.com/s/1fARfFl3jlfN2_Pb7VF-QNQ?pwd=143y">Segmentation Results (Baidu)</a>
                    <a class="btn btn-primary m-1" href="https://drive.google.com/file/d/1v3jE8Hr0_okEJgOMyf-pVAaLtgu89FjU/view?usp=drive_link">Comparison Results (Google Drive)</a>
                    <a class="btn btn-primary m-1" href="https://pan.baidu.com/s/1RH34CrRDQV4g_yyeV7j0FQ?pwd=pi6r">Comparison Results (Baidu)</a>
                    <a class="btn btn-primary m-1" href="https://drive.google.com/file/d/1Tvse5DIYFQa6e_dmnba_O0U7fKKin_TW/view?usp=drive_link">Evaluation Code</a>
                </div>
            </div>
        </div>
    </div>
    </br></br>


    <!-- <p class="paragraph-3 nerf_text" style="text-align: center;">
        The code and other resources will be made publicly available after the paper is accepted. Thank you! :)
    </p> -->

    <div class="affiliations" style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 15px;">
        <!-- <img src="logo/tongji.png" alt="UESTC Logo" style="height: 40px; width: auto;"> -->
        
    <p style="font-size: 1.1rem; color: #555; margin: 0;">
            More resources will be released upon acceptance :)
    </p>
    </div>

</div>



<div class="container">

    <div class="row">
        <div class="col-md-10 offset-md-1">
            <h2 class="text-center">Abstract</h2>
            <p class="text-justify" style="font-size: 1.1em; line-height: 1.6;">
                Audio-visual segmentation (AVS) requires precise pixel-level localization and classification guided by acoustic cues. However, existing methods often suffer from modality representation discrepancies and spurious correlations, where the model may over-rely on visual dominance rather than authentic audio-visual causality. In this work, we propose Omni-Implicit Counterfactual Learning (OICL), a unified causal inference paradigm for unbiased and fine-grained AVS. To bridge the gap between sparse audio and dense visual features, we first introduce Multi-granularity Implicit Text (MIT). By establishing a shared embedding space across video, segment, and frame levels, MIT provides a semantically grounded medium for cross-modal interaction. Building upon this, we develop a dual-pathway counterfactual intervention framework to disentangle causal dependencies. Specifically, Hierarchical Semantic Counterfactual (HSC) intervenes in the semantic space through a fast–slow mechanism, where implicit MIT cues enable rapid local adaptations while MLLM-driven descriptions provide stable context reasoning. Symmetrically, Hierarchical Perceptual Counterfactual (HPC) performs modality-aware interventions along audio and visual pathways, utilizing a similar fast–slow strategy to enhance robustness across diverse perceptual subspaces. These counterfactual constructions are optimized via Distribution-informed Cooperative Contrastive Learning (DCCL), a structured objective that jointly models factual–counterfactual, intra-modal, and inter-modal relationships, thereby suppressing spurious biases while promoting cohesive yet decoupled representations. Extensive experiments on six benchmarks demonstrate that OICL achieves state-of-the-art performance, with superior generalization in few-shot and open-vocabulary scenarios.
            </p>
        </div>
    </div>

    <hr>
    <h2 class="text-center">Demo Videos</h2>
    <div class="row">
        <div class="col-md-4">
            <p class="text-center">Bassoon</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/bassoon.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Marimba</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/marimba.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Airplane</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/airplane.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Harp</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/harp.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Keyboard</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/keyboard.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Guitar & Man</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/guitar_man.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Accordion & Guitar</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/accordion_guitar.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Cello & Violin</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/cello_violin.mp4" type="video/mp4">
            </video>
        </div>

        <div class="col-md-4">
            <p class="text-center">Cello & bassoon & Violin</p>
            <video width="100%" controls muted autoplay loop style="border-radius: 8px;">
                <source src="./videos/cello_bassoon_violin.mp4" type="video/mp4">
            </video>
        </div>

    </div>

    <hr>
    <div class="container">
        <h2>BibTeX</h2>
        <pre style="background-color: #f5f5f5; padding: 20px; border-radius: 5px;"><code>@article{zha2025oicl,
        title={Towards Omni-Implicit Counterfactual Learning in Audio-Visual Segmentation},
        author={Zha, Mingfeng and Wang, Guoqing and Li, Tianyu and Wang, Peng and Pei, Yunqiang and Guo, Jingcai and Yang, Yang and Shen, Heng Tao},
        journal={preprint},
        year={2026}
    }</code></pre>
    </div>

    </br></br>
    <div class="section">


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
