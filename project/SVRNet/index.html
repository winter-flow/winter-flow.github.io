<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Anpei Chen*,
                                Zexiang Xu*,
                                Andreas Geiger,
                                Jingyi Yu,
                                Hao Su">

    <title>Think Twice Before Determining: Towards Scene-aware Visual Reasoning for Mirror Detection</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">SVRNet</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Think Twice Before Determining: Towards Scene-aware Visual Reasoning for Mirror Detection</h1>
<!--    <h2>TensoRF: Tensorial Radiance Fields</h2>-->
        <h3 class="nerf_subheader_v2">IEEE TPAMI, Submitted</h3>
<!--            <p class="abstract">A compact and efficent scene representation</p>-->
    <hr>
    <p class="authors">
        <a href="https://winter-flow.github.io/">Mingfeng Zha</a>,
        <a href="https://scholar.google.com/citations?user=XzZSbxAAAAAJ&hl=en&oi=sra">Yunqiang Pei</a>,
        <a href="https://faculty.uestc.edu.cn/wangguoqing2/zh_CN/index.htm">Guoqing Wang*</a>,
        <a href="https://scholar.google.com/citations?user=6MUsCT4AAAAJ&hl=en&oi=sra">Tianyu Li</a>,
        <a href="https://teacher.ucas.ac.cn/~0066508">Xiongxin Tang</a>,
        <a href="https://scholar.google.com/citations?user=73trMQkAAAAJ&hl=en">Jiayi Ma</a>,
        <a href="https://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>,
        <a href="https://scholar.google.com.au/citations?user=krryaDkAAAAJ&hl=en">Heng Tao Shen</a>

    </p>

    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Corresponding Author</div>

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://ChengShiest.github.io/docs/02613.pdf">Paper</a>
        <a class="btn btn-primary" href="https://chengshiest.github.io/logo/">Dataset</a>
        <a class="btn btn-primary" href="https://github.com/winter-flow/WSMD">Experimental Results</a>
        <a class="btn btn-primary" href="https://github.com/winter-flow/WSMD">Code (upon acceptance)</a>
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Illustration</h2>
<!--    <div class="section">-->
        <div class="columns-5 w-row">
            <img src="imgs/Motivation and superiority.png" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>
        <p class="paragraph-3 nerf_text">
            Existing MD and related binary segmentation frameworks. (a) The naive encoding-decoding represented by HetNet; 
            (b) The dual-stream represented by SATNet; 
            (c) The explicit visual prompt represented by EVP; 
            (d) Our SVRNet. Comparison of our SVRNet with different types state-of-the-art (SOTA) detection methods on weighted F-measure, IoU, and parameters using the MSD dataset. 
            Larger circle indicates more parameters.
        </p>
    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Mirror detection (MD) aims to overcome interference caused by reflections and locate mirror regions. 
                Existing methods focus on designing components to explicitly establish the associations between physical entities and corresponding imagings, 
                or utilizing rotation to construct symmetric consistency. We observe that: a) only partial or none of entities and imagings are matched; 
                b) smooth surfaces (e.g., tiles) and transparent objects (e.g., glasses) also exhibit reflections; 
                c) occlusion and scale variation of mirror regions. To address these issues in a unified manner, 
                we formulate the scene-aware visual reasoning network (SVRNet) based on visual prompts. 
                Specifically, we design the prompt chain reasoning (PCR) that generates a chain of thought reasoning based on maximal difference heterogeneous prototypes to construct complex spatial location and semantic perception. 
                Noise may accumulate gradually through the chain, and crucial clues may also disappear. 
                Therefore, we introduce the prompt evolution (PE) to filter out noise and enhance the coupling between prompts. 
                We further propose the prompt injection expert (PIE) to dynamically select the optimal injection strategy in the low-rank space based on specific scene. 
                Due to reflection interference causing potential ambiguity in prompts and features, we propose the evidence-aware (EA) loss to quantify uncertainty, 
                thereby providing reliable predictions. Besides, we relabel 25,828 images to validate the model performance in the weakly supervised setting. 
                Extensive experiments on four mirror benchmarks demonstrate that our method surpasses state-of-the-art approaches. 
                Encouraging results are also achieved on six related benchmarks, showing its generality.

            </p>

            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Architecture comparison between (a) our deeplydecoupled EdaDet and (b) existing open-vocabulary detection
                framework. EdaDet separates the open-vocabulary classification
                branch from the class-agnostic proposal generation branch at a
                more shallow layer of the decoder. EdaDet first individually generates object proposals and predicts dense probabilities to categories
                for local image semantics and then computes object proposals' categories based on the dense probabilities.

            </p>
            <div class="columns-5 w-row">
                <img src="imgs/Method.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
        </div>
    </div>



    </br></br>
    <div class="section">
        <s2>Performance</s2>
        <hr>
        <h2 class="grey-heading_nerf">
            Quantitative comparison on four mirror datasets
        </h2>
        <div class="columns-5 w-row">
            <img src="imgs/Quantitative comparison on four mirror datasets.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>

        <h2 class="grey-heading_nerf">
                Visulization
        </h2>
        <p class="paragraph-3 nerf_text">
            Qualitative comparison on four mirror datasets
        </p>
        <div class="columns-5 w-row">
            <img src="imgs/Qualitative comparison on four mirror datasets.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>



        </br></br></br>

        </br></br>


        <s2> Acknowledgements </s2>
        <hr>
        <p class="paragraph-3 nerf_text">
            Thank you very much to Prof. Rynson W.H. Lau and his team for their series of work on mirror detection, from which we benefit greatly. 
            This work was supported in part by the National Natural Science Foundation of China under grant U23B2011, 
            62102069, U20B2063 and 62220106008, the Key R&D Program of Zhejiang under grant 2024SSYS0091.
        </p>



        <!-- <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
    @InProceedings{Shi_2023_ICCV,
        author = {Shi, Cheng and Yang, Sibei},
        title = {EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment},
        booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
        month = {October},
        year = {2023}
        }
        </div> -->


    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from <a href="https://apchenstu.github.io/TensoRF//">TensoRF</a>.
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
