<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Anpei Chen*,
                                Zexiang Xu*,
                                Andreas Geiger,
                                Jingyi Yu,
                                Hao Su">

    <title>EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">EdaDet</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Open-Vocabulary Object Detection Using Early Dense Alignment</h1>
<!--    <h2>TensoRF: Tensorial Radiance Fields</h2>-->
        <h3 class="nerf_subheader_v2">ICCV 2023</h3>
<!--            <p class="abstract">A compact and efficent scene representation</p>-->
    <hr>
    <p class="authors">
        <a href="https://ChengShiest.github.io"> Cheng Shi</a>,
        <a href="https://faculty.sist.shanghaitech.edu.cn/yangsibei/"> Sibei Yang*</a>

    </p>

    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Corresponding Author</div>

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://ChengShiest.github.io/docs/02613.pdf">Paper</a>
        <a class="btn btn-primary" href="https://chengshiest.github.io/logo/">Video(Coming Soon)</a>
        <a class="btn btn-primary" href="https://chengshiest.github.io/edadet/">Code(Coming Soon)</a>
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Illustration</h2>
<!--    <div class="section">-->
        <div class="columns-5 w-row">
            <img src="img/eda_p1.png" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>
        <p class="paragraph-3 nerf_text">
            Different approaches to building an open-vocabulary
            detector: (a) generate pseudo “novel” proposals from extra train-
            ing resources and VLMs, or (b) generalize from VLMs, and (c)
            their performance comparison. †: with self-training
        </p>
    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Vision-language models such as CLIP have boosted the
                performance of open-vocabulary object detection, where
                the detector is trained on base categories but required to
                detect novel categories. Existing methods leverage CLIP's
                strong zero-shot recognition ability to align object-level embeddings with textual embeddings of categories. However,
                we observe that using CLIP for object-level alignment results in overfitting to base categories, i.e., novel categories
                most similar to base categories have particularly poor performance as they are recognized as similar base categories.
                In this paper, we first identify that the loss of critical finegrained local image semantics hinders existing methods
                from attaining strong base-to-novel generalization. Then,
                we propose Early Dense Alignment (EDA) to bridge the gap
                between generalizable local semantics and object-level prediction. In EDA, we use object-level supervision to learn
                the dense-level rather than object-level alignment to maintain the local fine-grained semantics. Extensive experiments demonstrate our superior performance to competing
                approaches under the same strict setting and without using
                external training resources, i.e., improving the +8.4% novel
                box AP50 on COCO and +3.9% rare mask AP on LVIS.

            </p>
            <div class="columns-5 w-row">
                <img src="img/eda_p2.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
            <p class="paragraph-3 nerf_text">
                The comparison between Object-level Alignment and our Early Dense Alignment (Eda) on (a1)-(b1) architectures, (a2)-(b2)
                local image semantics and clustering results, and (c) box AP of novel categories similar to base categories. We list six novel categories
                most similar to base categories by calculating the average similarity between the randomly sampled thousands of novel objects' visual
                features and base categories' text embeddings. Our Eda: (1) successfully recognizes the fine-grained novel CD-player that is predicted to
                base speaker by object-level alignment; (2) better groups local image semantics into object regions compared with CLIP; (3) achieves a
                much higher novel box AP for predicting novel objects similar to base objects, showing that Eda can distinguish fine-grained details of
                similar novel and base categories. In contrast, object-level alignment overfits base categories.
            </p>

            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Architecture comparison between (a) our deeplydecoupled EdaDet and (b) existing open-vocabulary detection
                framework. EdaDet separates the open-vocabulary classification
                branch from the class-agnostic proposal generation branch at a
                more shallow layer of the decoder. EdaDet first individually generates object proposals and predicts dense probabilities to categories
                for local image semantics and then computes object proposals' categories based on the dense probabilities.

            </p>
            <div class="columns-5 w-row">
                <img src="img/eda_p3.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
        </div>
    </div>



    </br></br>
    <div class="section">
        <s2>Performance</s2>
        <hr>
        <h2 class="grey-heading_nerf">
            Open-vocabulary object detection results on COCO and LVIS datasets.
        </h2>
        <div class="columns-5 w-row">
            <img src="img/eda_p4.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>

        <h2 class="grey-heading_nerf">
                Visulization
        </h2>
        <p class="paragraph-3 nerf_text">
            We visualize our detection results and semantic maps on (a) LVIS [17], (b) COCO [29] and (c)
            out-of-distribution images from the open-source website.
        </p>
        <div class="columns-5 w-row">
            <img src="img/eda_p5.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>



        </br></br></br>
<!--    </div>-->

<!--        </br></br>-->
<!--        <s2>Arxiv</s2>-->
<!--        <hr>-->
<!--        <div>-->
<!--            <div class="list-group">-->
<!--                <a href="https://arxiv.org/abs/2103.15595"-->
<!--                   class="list-group-item">-->
<!--                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">-->
<!--                </a>-->
<!--            </div>-->
<!--        </div>-->

        </br></br>
<!--    <div class="section">-->
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
    @InProceedings{Shi_2023_ICCV,
        author = {Shi, Cheng and Yang, Sibei},
        title = {EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment},
        booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
        month = {October},
        year = {2023}
        }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from <a href="https://apchenstu.github.io/TensoRF//">TensoRF</a>.
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
